{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spotify Logo](https://storage.googleapis.com/pr-newsroom-wp/1/2018/11/Spotify_Logo_RGB_Green.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spotify Song Popularity Prediction\n",
    "\n",
    "This notebook implements a machine learning model to predict song popularity on Spotify using various audio features.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#1-setup-and-imports)\n",
    "2. [Data Loading and Exploration](#2-data-loading-and-exploration)\n",
    "3. [Data Preprocessing & Feature Engineering](#3-data-preprocessing--feature-engineering)\n",
    "4. [Model Training](#4-model-training)\n",
    "5. [Model Evaluation](#5-model-evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports <a id='setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_theme(style=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration <a id='data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/raw/spotify_songs.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df.drop(columns=['track_id', 'track_name', 'track_artist', 'track_album_id', 'track_album_name', 'playlist_id', 'playlist_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data info\n",
    "print(\"Dataset Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update data types\n",
    "# Define data types for each column\n",
    "dtypes = {\n",
    "    'track_popularity': 'int32',\n",
    "    'track_album_release_date': 'datetime64[ns]',\n",
    "    'playlist_genre': 'category', \n",
    "    'playlist_subgenre': 'category',\n",
    "    'danceability': 'float32',\n",
    "    'energy': 'float32',\n",
    "    'key': 'int8',\n",
    "    'loudness': 'float32', \n",
    "    'mode': 'category',\n",
    "    'speechiness': 'float32',\n",
    "    'acousticness': 'float32',\n",
    "    'instrumentalness': 'float32',\n",
    "    'liveness': 'float32',\n",
    "    'valence': 'float32',\n",
    "    'tempo': 'float32',\n",
    "    'duration_ms': 'int32'\n",
    "}\n",
    "\n",
    "# Update data types\n",
    "df = df.astype(dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: **'Key'** is a categorical variable but is encoded as an integer. More on this in the [Data Preprocessing & Feature Selection](#3-data-preprocessing--feature-engineering) section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of unique categories for categorical variables\n",
    "print(\"Number of unique categories in categorical variables:\")\n",
    "categorical_features = df.select_dtypes(include=['category']).columns\n",
    "for col in categorical_features:\n",
    "    print(f\"{col}: {df[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistical description\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset appears to have no missing values. However, according to the data documentation, the `key` column has missing values encoded as `-1`. We will address this in the next section: [Data Preprocessing & Feature Selection](#3-data-preprocessing--feature-engineering).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a distribution plot for each numeric column\n",
    "df_numerical_features = df.select_dtypes(include=['int8', 'int32', 'float32']).columns\n",
    "\n",
    "n_rows = (len(df_numerical_features) + 1) // 3\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=3, figsize=(15, 4 * n_rows))\n",
    "axes = axes.flatten() \n",
    "\n",
    "for i, (ax, col) in enumerate(zip(axes, df_numerical_features)):\n",
    "    sns.histplot(data=df, x=col, kde=True, ax=ax)\n",
    "    ax.set_title(f'Distribution of {col}')\n",
    "\n",
    "# Hide empty subplots if odd number of features\n",
    "if len(df_numerical_features) % 2 != 0:\n",
    "    axes[-1].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers in numerical columns\n",
    "\n",
    "n_rows = (len(df_numerical_features) + 1) // 3\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=3, figsize=(15, 4 * n_rows))\n",
    "axes = axes.flatten() \n",
    "\n",
    "for i, (ax, col) in enumerate(zip(axes, df_numerical_features)):\n",
    "    sns.boxplot(data=df, x=col, ax=ax)\n",
    "    ax.set_title(f'Boxplot of {col}')\n",
    "\n",
    "# Hide empty subplots if odd number of features\n",
    "if len(df_numerical_features) % 3 != 0:\n",
    "    for ax in axes[len(df_numerical_features):]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do we need to clean the data for outliers?\n",
    "\n",
    "1. **Tree-based models (XGBoost, LightGBM, CatBoost) are robust to outliers** since they split data into bins rather than fitting continuous functions.  \n",
    "2. **Large dataset size minimizes the impact of extreme values**, making explicit outlier treatment unnecessary.  \n",
    "3. **Outliers may contain valuable insights**, and removing them risks losing important patterns.  \n",
    "4. **Categorical data is unaffected by outliers**, and encoding methods handle variations naturally.  \n",
    "5. **Regularization and loss functions manage extreme values**, reducing their influence without manual removal.  \n",
    "\n",
    "Given these factors, outlier cleaning is unnecessary for this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical features\n",
    "correlation_matrix = df[df_numerical_features].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visulize the relationship between numerical features and the target variable\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, feature in enumerate(df_numerical_features):\n",
    "    plt.subplot(5, 3, i + 1)\n",
    "    sns.scatterplot(data=df, x=feature, y='track_popularity')\n",
    "    plt.title(f'{feature} vs Popularity')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing & Feature Engineering <a id='preprocessing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Cyclical Variables (Key & Month) with Sine & Cosine\n",
    "\n",
    "**Key Definition**  \n",
    "**`key`**: The estimated overall key of the track (0 = C, 1 = C♯/D♭, ..., 11 = B). If no key is detected, the value is `-1`.\n",
    "\n",
    "Both **`key`** (0–11) and **release_month** (1–12) are naturally **cyclical**. To preserve adjacency (e.g., key 0 near key 11; month 12 near month 1), we encode them with **sine** and **cosine**:\n",
    "\n",
    "$$\n",
    "\\text{key\\_sin} = \\sin\\!\\Bigl(2\\pi \\cdot \\frac{\\text{key}}{12}\\Bigr), \\quad\n",
    "\\text{key\\_cos} = \\cos\\!\\Bigl(2\\pi \\cdot \\frac{\\text{key}}{12}\\Bigr);\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{release\\_month\\_sin}(m) = \\sin\\!\\Bigl(\\frac{2\\pi \\times m}{12}\\Bigr), \\quad\n",
    "\\text{release\\_month\\_cos}(m) = \\cos\\!\\Bigl(\\frac{2\\pi \\times m}{12}\\Bigr).\n",
    "$$\n",
    "\n",
    "**Why Both Sine & Cosine?**  \n",
    "1. **2D Representation**  \n",
    "   A single trigonometric function (e.g., only sine) flattens the circle to a line, losing adjacency (e.g., January next to December). Using **both** sine and cosine maps each key/month to a **unique 2D point**, preserving cyclical distance.\n",
    "\n",
    "2. **Avoiding Ambiguities**  \n",
    "   Sine alone can yield the same magnitude for angles \\(\\theta\\) and \\(\\theta+\\pi\\). Adding cosine disambiguates these positions, preventing overlap.\n",
    "\n",
    "**Handling `-1` for Key**  \n",
    "Tracks with `key = -1` (no key detected) should have their `key_sin` and `key_cos` set to `NaN`, ensuring they don’t overlap with valid keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_engineer_features(df):\n",
    "    \"\"\"\n",
    "    Preprocess and engineer new features for the Spotify dataset.\n",
    "    \"\"\"\n",
    "    df_featured = df.copy()\n",
    "    \n",
    "    # Encode key with sine & cosine\n",
    "    if 'key' in df_featured.columns:\n",
    "        df_featured['key_sin'] = np.sin(2 * np.pi * df_featured['key'] / 12)\n",
    "        df_featured['key_cos'] = np.cos(2 * np.pi * df_featured['key'] / 12)\n",
    "        df_featured.loc[df_featured['key'] == -1, ['key_sin', 'key_cos']] = np.nan\n",
    "        df_featured.drop(columns='key', inplace=True)\n",
    "\n",
    "    # Audio feature engineering\n",
    "    if all(col in df_featured.columns for col in ['energy', 'danceability']):\n",
    "        df_featured['energy_dance_ratio'] = df_featured['energy'] / df_featured['danceability']\n",
    "        df_featured['energy_dance_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    if all(col in df_featured.columns for col in ['acousticness', 'energy']):\n",
    "        df_featured['acoustic_energy_balance'] = df_featured['acousticness'] - df_featured['energy']\n",
    "    \n",
    "    if all(col in df_featured.columns for col in ['valence', 'energy']):\n",
    "        df_featured['mood_score'] = df_featured['valence'] * df_featured['energy']\n",
    "    \n",
    "    # Tempo categories\n",
    "    if 'tempo' in df_featured.columns:\n",
    "        df_featured['tempo_category'] = pd.qcut(df_featured['tempo'], 4, \n",
    "                                              labels=['slow', 'medium', 'fast', 'very_fast'])\n",
    "\n",
    "    # Release date features\n",
    "    if 'track_album_release_date' in df_featured.columns:\n",
    "        df_featured['track_album_release_date'] = pd.to_datetime(df_featured['track_album_release_date'], \n",
    "                                                               errors='coerce')\n",
    "        \n",
    "        df_featured['release_year'] = df_featured['track_album_release_date'].dt.year\n",
    "        df_featured['release_month'] = df_featured['track_album_release_date'].dt.month\n",
    "        df_featured['release_day'] = df_featured['track_album_release_date'].dt.day\n",
    "        \n",
    "        # Cyclical month encoding\n",
    "        month = 2 * np.pi * df_featured['release_month'] / 12\n",
    "        df_featured['release_month_sin'] = np.sin(month)\n",
    "        df_featured['release_month_cos'] = np.cos(month)\n",
    "\n",
    "        df_featured['track_age_years'] = datetime.now().year - df_featured['release_year']\n",
    "        \n",
    "        df_featured.drop(columns='track_album_release_date', inplace=True)\n",
    "\n",
    "    return df_featured\n",
    "\n",
    "# Engineer features\n",
    "df_featured = preprocess_and_engineer_features(df)\n",
    "\n",
    "# Display new features\n",
    "print(\"New Features:\")\n",
    "new_features = ['key_sin', 'key_cos', 'energy_dance_ratio', 'acoustic_energy_balance', 'mood_score', 'tempo_category', 'release_month_sin', 'release_month_cos']\n",
    "df_featured[new_features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training <a id='training'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featured.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df_featured.drop(columns='track_popularity')\n",
    "y = df_featured['track_popularity']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "df_f_numeric_features = X.select_dtypes(include=['int32', 'float32', 'float64']).columns\n",
    "df_f_categorical_features = X.select_dtypes(include=['category']).columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model with categorical features\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('preprocessing', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), df_f_numeric_features),\n",
    "            ('cat', OneHotEncoder(sparse_output=False), df_f_categorical_features)\n",
    "        ]\n",
    "    )),\n",
    "    ('regressor', xgb.XGBRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "xgb_params = {\n",
    "    'regressor__n_estimators': [100, 200],\n",
    "    'regressor__max_depth': [3, 5, 7],\n",
    "    'regressor__learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb_pipeline, xgb_params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best XGBoost parameters:\", xgb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM model\n",
    "lgb_pipeline = Pipeline([\n",
    "    ('preprocessing', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), df_f_numeric_features),\n",
    "            ('cat', OneHotEncoder(sparse_output=False), df_f_categorical_features)\n",
    "        ]\n",
    "    )),\n",
    "    ('regressor', lgb.LGBMRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "lgb_params = {\n",
    "    'regressor__n_estimators': [100, 200],\n",
    "    'regressor__num_leaves': [31, 63],\n",
    "    'regressor__learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "lgb_grid = GridSearchCV(lgb_pipeline, lgb_params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "lgb_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best LightGBM parameters:\", lgb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CatBoost model\n",
    "cat_pipeline = Pipeline([\n",
    "    ('preprocessing', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), df_f_numeric_features),\n",
    "            ('cat', 'passthrough', df_f_categorical_features)\n",
    "        ]\n",
    "    )),\n",
    "    ('regressor', CatBoostRegressor(\n",
    "        random_state=42, \n",
    "        verbose=False, \n",
    "        task_type=\"CPU\",\n",
    "        cat_features=[i + len(df_f_numeric_features) for i in range(len(df_f_categorical_features))]\n",
    "    ))\n",
    "])\n",
    "\n",
    "cat_params = {\n",
    "    'regressor__iterations': [100, 200],\n",
    "    'regressor__depth': [4, 6],\n",
    "    'regressor__learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "cat_grid = GridSearchCV(cat_pipeline, cat_params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "cat_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best CatBoost parameters:\", cat_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation <a id='evaluation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation function\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Evaluate model performance with comprehensive metrics.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Basic metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals = y_test - y_pred\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance Metrics:\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"R² Score: {r2:.3f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"Residual Range: [{residuals.min():.2f}, {residuals.max():.2f}]\")\n",
    "    \n",
    "    return y_pred, residuals\n",
    "\n",
    "# Evaluate the three models \n",
    "models = {\n",
    "    'XGBoost': xgb_grid,\n",
    "    'LightGBM': lgb_grid,\n",
    "    'CatBoost': cat_grid}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparisons function\n",
    "def plot_model_comparisons(models, X_test, y_test):\n",
    "    \"\"\"Create comprehensive comparison visualizations for multiple models.\"\"\"\n",
    "    # Store results\n",
    "    results = {}\n",
    "    \n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Evaluate each model\n",
    "    for i, (model_name, model) in enumerate(models.items()):\n",
    "        y_pred, residuals = evaluate_model(model, X_test, y_test, model_name)\n",
    "        results[model_name] = {'y_pred': y_pred, 'residuals': residuals}\n",
    "        \n",
    "        # Actual vs Predicted plot\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "        plt.title(f'{model_name}: Actual vs Predicted')\n",
    "        plt.xlabel('Actual Popularity')\n",
    "        plt.ylabel('Predicted Popularity')\n",
    "        \n",
    "        # Residuals plot\n",
    "        plt.subplot(3, 3, i+4)\n",
    "        plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "        plt.axhline(y=0, color='r', linestyle='--')\n",
    "        plt.title(f'{model_name}: Residuals')\n",
    "        plt.xlabel('Predicted Popularity')\n",
    "        plt.ylabel('Residuals')\n",
    "        \n",
    "        # Residual distribution\n",
    "        plt.subplot(3, 3, i+7)\n",
    "        sns.histplot(residuals, kde=True)\n",
    "        plt.axvline(x=0, color='r', linestyle='--')\n",
    "        plt.title(f'{model_name}: Residual Distribution')\n",
    "        plt.xlabel('Residual Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Plot model comparisons\n",
    "plot_model_comparisons(models, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
